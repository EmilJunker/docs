\documentclass[paper=a4]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{url}

\title{Overview of IDS Internals}
\author{Rolf Krahl}

% Add a revision hint as a unnumbered footnote
\newcommand{\revhint}{%
  \begingroup%
  \let\thefootnote\relax%
  \footnote{Revision: \input{.revision}}%
  \addtocounter{footnote}{-1}%
  \endgroup%
}

\begin{document}

\maketitle

\section{Introduction}

\revhint{}%
The ICAT Data Service (IDS) operates with delayed actions, internal
states, and multiple threads.  This makes it somewhat difficult to
understand from mere reading the sources what actions may be performed
under certain conditions.  The knowledge of the internal processes in
IDS may be needed to properly implement a storage plugin in a
non-trivial setup.  In particular, one might need to know the context
that each of the plugin's methods may be called in.  This text shall
provide a reference to accommodate this need.  It is however only an
overview.  In order to keep the representation clear, many details
have been omitted.  Furthermore it is somewhat biased towards the
point of view of a storage plugin developer and it is restricted to
the case that the IDS is configured as a two level storage with the
storage unit set to dataset.  The text is based on IDS server 1.3.1.

The workflow in the IDS can be sketched as follows: the IDS waits for
incoming service requests coming from the user.  In some cases, these
requests may be completed immediately.  In other cases, a deferred
operation that will be processed in the background later is queued.

Before describing this workflow in detail, we give the definition of
the status of a dataset in Section \ref{sec:states}.  Section
\ref{sec:requests} lists the service request and what is done in each
case.  The various operations that are queued for any given dataset
are kept track of in a finite state machine.  This is detailed in
Section \ref{sec:fsm}.  When processing the queue, each of the
deferred operations is executed in a separate thread.  Section
\ref{sec:defops} describes what is being done for each of the
operations.  Besides processing service requests, some maintenance
task are running regularly in the background.  These tasks are
described in Section \ref{sec:maintenance}.  Finally, we provide an
index of the context that each storage plugin method is called from in
Section \ref{sec:plugincalls}.


\section{Internal states}
\label{sec:states}

The processing of a data object is influenced by its internal state
and what operations have been queued for them.  We say that an
operation is \emph{in process} on a dataset if this operation is
either in the queue or if it is currently being executed for that
dataset.

The status of a dataset may either be \texttt{ONLINE},
\texttt{ARCHIVED}, or \texttt{RESTORING}.  It is \texttt{RESTORING} if
a \texttt{RESTORE} operation on that dataset is in process.  If this
is not the case, the status is \texttt{ARCHIVED} if any other
operation then \texttt{WRITE} is in process or if the dataset is not
empty (e.g. related datafiles exist), but the dataset directory does
not exist in the main storage.  In all other cases, the status is
\texttt{ONLINE}.


\section{Service requests}
\label{sec:requests}

In this section, we consider each IDS request and describe in detail
what is done in each case.  We consider only requests that interact in
any way with the storage plugin.  The requests \texttt{getApiVersion},
\texttt{getServiceStatus}, \texttt{getSize} \texttt{isReadOnly},
\texttt{isTwoLevel}, and \texttt{ping} are thus skipped.

Several service calls that deal with a selection of data objects in
the storage expect lists of datafile, dataset, and investigation ids
as parameter.  Most internal processing is done at dataset level and
it only matters which datasets are concerned.  In these cases, we use
the term \emph{selected datasets} regardless whether the objects have
actually been selected by datafile, dataset, or investigation id.  In
a similar manner, we use the term \emph{selected datafiles}.

\subsection{\texttt{archive}}

Queue an \texttt{ARCHIVE} deferred operation for each of the selected
datasets.

\subsection{\texttt{delete}}
\label{sec:requests:delete}

If any of the selected datasets is not \texttt{ONLINE}
(Sec.\ \ref{sec:states}), queue a \texttt{RESTORE} deferred operation
for them and throw a \texttt{DataNotOnlineException}.

Otherwise delete the selected datafiles from ICAT and from the main
storage.  Finally queue a \texttt{WRITE} deferred operation for each
of the selected datasets.

\subsection{\texttt{getData}}
\label{sec:requests:getdata}

If any of the selected datasets is not \texttt{ONLINE}
(Sec.\ \ref{sec:states}), queue a \texttt{RESTORE} deferred operation
for them and throw a \texttt{DataNotOnlineException}.

Otherwise get the selected datafiles from the main storage and stream
their content to the client.

\subsection{\texttt{getLink}}
\label{sec:requests:getlink}

If the dataset related to the selected datafile is not \texttt{ONLINE}
(Sec.\ \ref{sec:states}), queue a \texttt{RESTORE} deferred operation
for it and throw a \texttt{DataNotOnlineException}.

Otherwise get the path of the file from main storage, set an ACL to
grant read permission to the user on the file, and create a link to
the file.

\subsection{\texttt{getStatus}}

Return \texttt{ONLINE} if all selected datasets are \texttt{ONLINE}
(Sec.\ \ref{sec:states}), otherwise return either \texttt{ARCHIVED} or
\texttt{RESTORING}.

\subsection{\texttt{isPrepared}}

If any of the selected datasets (as stored in the previously prepared
data selection) is not \texttt{ONLINE} (Sec.\ \ref{sec:states}), queue
a \texttt{RESTORE} deferred operation for them and return
\texttt{false}.

Otherwise return \texttt{true}.

\subsection{\texttt{prepareData}}

If any of the selected datasets is not \texttt{ONLINE}
(Sec.\ \ref{sec:states}), queue a \texttt{RESTORE} deferred operation
for them.  Store the data selection and other parameter of the request
and return the prepared id for later referral.

\subsection{\texttt{put}}
\label{sec:requests:put}

If the dataset referenced in the request is not \texttt{ONLINE}
(Sec.\ \ref{sec:states}), queue a \texttt{RESTORE} deferred operation
for it and throw a \texttt{DataNotOnlineException}.

Otherwise store the uploaded file in the main storage, create the
datafile object in ICAT, and queue a \texttt{WRITE} deferred operation
for the dataset.

\subsection{\texttt{restore}}

Queue an \texttt{RESTORE} deferred operation for each of the selected
datasets.


\section{Finite state machine}
\label{sec:fsm}

The finite state machine manages the queue of deferred operations for
each dataset.  The operation may be changed if another operation is
queued for the same dataset while the previous one is still waiting in
the queue, see Tab.\ \ref{tab:fsm} for the update matrix.
\begin{table}
  \centering
  \begin{tabular}{l|lll}
                                  & \texttt{ARCHIVE} & \texttt{RESTORE} & \texttt{WRITE} \\
    \hline
    none                          & \texttt{ARCHIVE} & \texttt{RESTORE} & \texttt{WRITE} \\
    \texttt{ARCHIVE}              & \texttt{ARCHIVE} & \texttt{RESTORE} & \texttt{WRITE\_THEN\_ARCHIVE} \\
    \texttt{RESTORE}              & \texttt{ARCHIVE} & \texttt{RESTORE} & \texttt{WRITE} \\
    \texttt{WRITE}                & \texttt{WRITE\_THEN\_ARCHIVE} & \texttt{WRITE} & \texttt{WRITE} \\
    \texttt{WRITE\_THEN\_ARCHIVE} & \texttt{WRITE\_THEN\_ARCHIVE} & \texttt{WRITE} & \texttt{WRITE\_THEN\_ARCHIVE} \\
  \end{tabular}
  \caption{Update matrix for queued operations.  Matrix entries are
    the result if a new operation (column) is queued while a previous
    one (row) is still waiting in the queue.}
  \label{tab:fsm}
\end{table}

The queue is processed regularly by a timer task that starts a new
thread to execute each of the pending operations.  The execution of a
queued operation for a dataset is hold back while a previous operation
for the same dataset is currently being executed.  \texttt{WRITE}
operations are processed with a delay that is fixed when the operation
is queued.  Any subsequent \texttt{WRITE} operation queued for the
same dataset pushes this delay further.


\section{Deferred operations}
\label{sec:defops}

When the queue of deferred operations is processed, a new thread is
started to execute each of them.  This section describes what these
threads do in each case.

\subsection{\texttt{ARCHIVE}}
\label{sec:defops:archive}

Delete the dataset from the main storage.

\subsection{\texttt{RESTORE}}
\label{sec:defops:restore}

Get the ZIP file of the dataset from the archive storage, extract the
datafiles from it, and store the datafiles in the main storage.

\subsection{\texttt{WRITE}}
\label{sec:defops:write}

If the dataset directory does not exist in the main storage, delete
the ZIP file of the dataset from archive storage.

Otherwise create a new ZIP file, get all datafiles that belong to the
dataset (according to ICAT) from the main storage and add them to the
ZIP file.  Store this ZIP file in the archive storage.

\subsection{\texttt{WRITE\_THEN\_ARCHIVE}}
\label{sec:defops:writethenarchive}

As the name suggests, same as \texttt{WRITE} and then
\texttt{ARCHIVE}:

If the dataset directory does not exist in the main storage, delete
the ZIP file of the dataset from archive storage.

Otherwise create a new ZIP file, get all datafiles that belong to the
dataset (according to ICAT) from the main storage and add them to the
ZIP file.  Store this ZIP file in the archive storage.  Delete the
dataset from the main storage.


\section{Maintenance tasks}
\label{sec:maintenance}

There are some maintenance tasks running in the background
independently from user actions.

\subsection{\texttt{FileChecker}}
\label{sec:maintenance:filechecker}

Iterate over all datasets in the ICAT including their datafiles.  For
each dataset, get the ZIP file from the archive storage and inspect it
to check that the list of datafiles in the ZIP file matches the list
of datafiles in the dataset and that length and checksum matches for
each of the datafiles.

\subsection{\texttt{Tidier}}
\label{sec:maintenance:tidier}

Query the main storage plugin for a list of datasets to remove in
order to get the overall size of the main storage below the configured
limit.  Queue an \texttt{ARCHIVE} deferred operation for each of the
returned datasets.


\section{Index of plugin method calls}
\label{sec:plugincalls}

In this section, we list for each of the storage plugin methods the
context that it is called from.  Methods not listed here are never
called in the considered configuration.

\subsection{\texttt{archiveStorage.delete(DsInfo)}}

Called from the \texttt{DsWriter} (Sec.\ \ref{sec:defops:write}) and
the \texttt{DsWriteThenArchiver}
(Sec.\ \ref{sec:defops:writethenarchive}) if the dataset directory
does not exist in the main storage.

\subsection{\texttt{archiveStorage.get(DsInfo, Path)}}

Called from the \texttt{DsRestorer} (Sec.\ \ref{sec:defops:restore})
to extract the ZIP file from the archive storage into main storage.
Called from the \texttt{FileChecker}
(Sec. \ref{sec:maintenance:filechecker}) to inspect the ZIP file.

\subsection{\texttt{archiveStorage.put(DsInfo, InputStream)}}

Called from the \texttt{DsWriter} (Sec.\ \ref{sec:defops:write}) and
the \texttt{DsWriteThenArchiver}
(Sec.\ \ref{sec:defops:writethenarchive}) to store the ZIP file in the
archive storage.

\subsection{\texttt{mainStorage.exists(DsInfo)}}

Called from \texttt{IdsBean} while processing various service requests
(Sec.\ \ref{sec:requests}) to determine the status of a dataset.
Called from the \texttt{DsWriter} (Sec.\ \ref{sec:defops:write}) and
the \texttt{DsWriteThenArchiver}
(Sec.\ \ref{sec:defops:writethenarchive}) to check if the dataset
directory exists.

\subsection{\texttt{mainStorage.exists(String)}}

Called from \texttt{IdsBean} while processing a \texttt{delete}
request (Sec.\ \ref{sec:requests:delete}) to check if a selected
datafile exists in the main storage before deleting it.

\subsection{\texttt{mainStorage.delete(DsInfo)}}

Called from the \texttt{DsArchiver} (Sec.\ \ref{sec:defops:archive})
and the \texttt{DsWriteThenArchiver}
(Sec.\ \ref{sec:defops:writethenarchive}) to delete the dataset from
the main storage.

\subsection{\texttt{mainStorage.delete(String, String, String)}}

Called from \texttt{IdsBean} while processing a \texttt{delete}
request (Sec.\ \ref{sec:requests:delete}) to delete the selected
datafiles from the main storage.  Called from \texttt{IdsBean} while
processing a \texttt{put} request (Sec.\ \ref{sec:requests:put}) to
delete the uploaded file from the main storage again in the case that
an error occurred while creating the correspoding datafile object in
ICAT.

\subsection{\texttt{mainStorage.get(String, String, String)}}

Called from \texttt{IdsBean} while processing a \texttt{getData}
request (Sec.\ \ref{sec:requests:getdata}) to get the selected
datafiles from the main storage.  Called from the \texttt{DsWriter}
(Sec.\ \ref{sec:defops:write}) and the \texttt{DsWrite\-Then\-Archiver}
(Sec.\ \ref{sec:defops:writethenarchive}) to get the datafiles and add
them to the ZIP archive.

\subsection{\texttt{mainStorage.getPath(String, String, String)}}

Called from \texttt{IdsBean} while processing a \texttt{getLink}
request (Sec.\ \ref{sec:requests:getlink}) to get the path of the file
from main storage.

\subsection{\texttt{mainStorage.put(DsInfo, String, InputStream)}}

Called from \texttt{IdsBean} while processing a \texttt{put} request
(Sec.\ \ref{sec:requests:put}) to store the uploaded file in the main
storage.

\subsection{\texttt{mainStorage.put(InputStream, String)}}

Called from the \texttt{DsRestorer} (Sec.\ \ref{sec:defops:restore})
to store the file extracted from the ZIP into main storage.

\subsection{\texttt{mainStorage.getDatasetsToArchive(long, long)}}

Called from the \texttt{Tidier} (Sec. \ref{sec:maintenance:tidier}) to
query the main storage plugin for a list of datasets to remove.


\end{document}


